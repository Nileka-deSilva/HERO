{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Derive Top N Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional DAILYTOPN-libraries in ETP-package of ASSETS-module imported successfully!\n",
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional DAILYTOPN-libraries in ETP-package of ASSETS-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional SPARKNOSQLWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "dailyTopN Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from utils.modules.etl.load import sparkDBwls as sdb\n",
    "# from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from mining.modules.assets.etp import dailyTopN as topN\n",
    "# from utils.modules.ml.timeseries import rollingstats as stats\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    topN = importlib.reload(topN)\n",
    "    sdb = importlib.reload(sdb)\n",
    "#     scne = importlib.reload(scne)\n",
    "#     stats= importlib.reload(stats)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "# clsSCNR=scne.Transformer(desc=__desc__)\n",
    "clsMPT =topN.WeightedPortfolio(desc=__desc__)\n",
    "# clsStat=stats.RollingStats(desc=__desc__)\n",
    "''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dd26e",
   "metadata": {},
   "source": [
    "## Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, retrieving data ...\n",
      "23/03/07 08:11:37 WARN Utils: Your hostname, FarmRaiderTester resolves to a loopback address: 127.0.1.1; using 192.168.124.15 instead (on interface enp2s0)\n",
      "23/03/07 08:11:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/07 08:11:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/07 08:11:39 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem Unable to get public no-arg constructor\n",
      "23/03/07 08:11:39 WARN FileSystem: java.lang.NoClassDefFoundError: com/google/api/client/auth/oauth2/Credential\n",
      "23/03/07 08:11:39 WARN FileSystem: java.lang.ClassNotFoundException: com.google.api.client.auth.oauth2.Credential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10908 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "_from_date = '2022-01-05'\n",
    "_to_date = '2022-01-10'\n",
    "_db_name = \"warehouse.mcap_past\"\n",
    "_kwargs={}\n",
    "# _query = \"select * from warehouse.mcap_past \"+\\\n",
    "#         f\"where mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "#         f\"and mcap_value > 1000000 and asset_name in \"+\\\n",
    "#         \"('brg_x','_crdn','avme','atri','ethix','hoge','xpx','wabi','dmg','mintme','chart')\"\n",
    "_query =f\"select * from {_db_name} \"+\\\n",
    "        f\"where mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "        f\"and mcap_value > 1000000 and log_ror is not null\"\n",
    "\n",
    "mcap_sdf = clsMPT.read_ror(select=_query,**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fdece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "|mcap_past_pk|          mcap_date|asset_name|          mcap_value|           log_ror|\n",
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "|       16328|2022-01-05 00:00:00|     audio|851781048.4787650...|0.6928591390799810|\n",
      "|       63439|2022-01-05 00:00:00|      dose|8672474.266345850...|0.6763315876313835|\n",
      "|       97873|2022-01-05 00:00:00|       kus|3176017.219590980...|0.6998238419681013|\n",
      "|       39730|2022-01-05 00:00:00|      drct|2244212.912972310...|0.6575184485114410|\n",
      "|       28910|2022-01-05 00:00:00|       l3p|1317788.022696750...|0.7187257420450860|\n",
      "|      151661|2022-01-05 00:00:00|        la|9276804.041408930...|0.6895376741222805|\n",
      "|        3694|2022-01-05 00:00:00|  babydoge|486963873.8483450...|0.6986166954553891|\n",
      "|       14630|2022-01-05 00:00:00|      dino|6450788.559264860...|0.6771463472778994|\n",
      "|       55686|2022-01-05 00:00:00|      kuma|13786922.58619880...|0.6959976142112440|\n",
      "|       43994|2022-01-05 00:00:00|     equad|7256520.205885230...|0.8270887390549453|\n",
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "mcap_sdf.select(\n",
    "    F.col('mcap_past_pk'),F.col('mcap_date'),\n",
    "    F.col('asset_name'),F.col('mcap_value'),F.col('log_ror'))\\\n",
    "    .sort(F.col('mcap_date'))\\\n",
    "    .show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f6f24",
   "metadata": {},
   "source": [
    "## Weighted Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9263fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'warehouse.mcap_past',\n",
       "  'mcap_past_pk': [91391, 124438, 78462],\n",
       "  'mcap_date': datetime.datetime(2022, 1, 9, 0, 0),\n",
       "  'asset_name': ['btcs', 'nabox', 'yaxis'],\n",
       "  'log_ror': [Decimal('2.9321703953454180'),\n",
       "   Decimal('1.9620722641637465'),\n",
       "   Decimal('1.4506834179632306')],\n",
       "  'weights': [0.6789370563680265, 0.06345221733977897, 0.25761072629219456],\n",
       "  'mcap_value': [Decimal('23756320.2128301000000000'),\n",
       "   Decimal('66498522.3461270000000000'),\n",
       "   Decimal('3036867.5895900100000000')]},\n",
       " {'source': 'warehouse.mcap_past',\n",
       "  'mcap_past_pk': [130969, 111435, 62127],\n",
       "  'mcap_date': datetime.datetime(2022, 1, 8, 0, 0),\n",
       "  'asset_name': ['l2', 'falcx', 'dana'],\n",
       "  'log_ror': [Decimal('1.5337095512357850'),\n",
       "   Decimal('1.3188759637834600'),\n",
       "   Decimal('1.2139873905404268')],\n",
       "  'weights': [0.6042672952906084, 0.14590263443562979, 0.24983007027376175],\n",
       "  'mcap_value': [Decimal('10755738.7793784000000000'),\n",
       "   Decimal('2651804.8528023700000000'),\n",
       "   Decimal('26502663.9578732000000000')]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cols={\n",
    "    \"PRIMARYKEY\":'mcap_past_pk',\n",
    "    \"NAMECOLUMN\":'asset_name',\n",
    "    \"DATECOLUMN\":'mcap_date',\n",
    "    \"NUMCOLUMN\" :'log_ror',\n",
    "    \"MCAPCOLUMN\":'mcap_value',\n",
    "    \"WEIGHTCOLUMN\":'weights',\n",
    "    \"MCAPSOURCE\":'source',\n",
    "}\n",
    "_l_exp_wts,_cols_dict=clsMPT.get_weighted_mpt(\n",
    "    data=mcap_sdf,\n",
    "    cols_dict=_cols,\n",
    "    topN=3,\n",
    "    size=5,\n",
    "    **_kwargs,\n",
    ")\n",
    "_l_exp_wts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97eaaeb",
   "metadata": {},
   "source": [
    "## Write MPT to MongoDB\n",
    "\n",
    "* Collection name = \"mpt.\"+date(YYYY-MM-DD)\n",
    "* document structure: \\_id, date, asset, mcap.value, mcap.weight, mcap.ror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccad5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert 6 documents\n"
     ]
    }
   ],
   "source": [
    "_kwargs = {\n",
    "    \"DESTINDBNAME\":'tip-daily-mpt',\n",
    "    \"COLLPREFIX\" : 'mpt'\n",
    "}\n",
    "mpt_list_ = clsMPT.write_mpt_to_db(\n",
    "    mpt_data=_l_exp_wts,\n",
    "    cols_dict=_cols,\n",
    "    **_kwargs,\n",
    ")\n",
    "print(\"Upsert %d documents\" % len(mpt_list_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb99563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

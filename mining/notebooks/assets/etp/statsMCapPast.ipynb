{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Read n Clean MCap with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional LOGRETURNS-libraries in ETP-package of ASSETS-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "logReturns Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from utils.modules.etl.load import sparkDBwls as sdb\n",
    "from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from mining.modules.assets.etp import logReturns as log\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    log = importlib.reload(log)\n",
    "    sdb = importlib.reload(sdb)\n",
    "    scne = importlib.reload(scne)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "# clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "clsSCNR = scne.Transformer(desc=__desc__)\n",
    "clsROR = log.RatioOfReturns(desc=__desc__)\n",
    "''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dd26e",
   "metadata": {},
   "source": [
    "## Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, retrieving data ...\n",
      "23/01/12 12:08:19 WARN Utils: Your hostname, FarmRaiderTester resolves to a loopback address: 127.0.1.1; using 192.168.124.15 instead (on interface enp2s0)\n",
      "23/01/12 12:08:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/12 12:08:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/12 12:08:21 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem Unable to get public no-arg constructor\n",
      "23/01/12 12:08:21 WARN FileSystem: java.lang.NoClassDefFoundError: com/google/api/client/auth/oauth2/Credential\n",
      "23/01/12 12:08:21 WARN FileSystem: java.lang.ClassNotFoundException: com.google.api.client.auth.oauth2.Credential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 12:10:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 12:10:53 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 12:11:23 WARN DAGScheduler: Broadcasting large task binary with size 6.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31 rows and 1742 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_from_date = '2022-01-01'\n",
    "_to_date = '2022-01-31'\n",
    "# _query = \"select * from warehouse.mcap_past \"+\\\n",
    "#         f\"where mcap_date >= '{_from_date}' and \"+\\\n",
    "#         f\"mcap_date <= '{_to_date}'\"\n",
    "_query = \"select * from warehouse.mcap_past \"+\\\n",
    "        f\"where mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "        f\"and mcap_value > 1000000\"\n",
    "_kwargs = {\n",
    "    \"TABLENAME\":'warehouse.mcap_past',\n",
    "    \"COLUMN\":'mcap_date',\n",
    "    \"FROMDATETIME\":_from_date,\n",
    "    \"TODATETIME\":_to_date,\n",
    "    \"PARTITIONS\":2,\n",
    "    \"AGGREGATE\":'avg',\n",
    "    \"PIVCOLUMNS\":['cofix','paypolitan-token','raven-protocol',\n",
    "               'nft-index','beldex','mt-pelerin-shares']\n",
    "}\n",
    "\n",
    "# print(clsSpark.dbSchema)\n",
    "mcap_sdf = clsROR.read_n_clean_mcap(query=_query,**_kwargs)\n",
    "# mcap_sdf = clsROR.read_n_clean_mcap(**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b887a",
   "metadata": {},
   "source": [
    "## Unpivot cleaned mcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8af85368",
   "metadata": {},
   "outputs": [],
   "source": [
    "_col_subset = mcap_sdf.columns\n",
    "_col_subset.remove('mcap_date')\n",
    "_unpivot_sdf = clsSCNR.unpivot_table(\n",
    "    table = mcap_sdf,\n",
    "    unpivot_columns=_col_subset,\n",
    "    index_column='mcap_date',\n",
    "    value_columns=['asset_name','mcap_value'],\n",
    "    where_cols = 'mcap_value',\n",
    "    **_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84b83274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 18:23:17 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 18:24:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 68:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clsSCNR = scne.Transformer(desc=__desc__)\n",
    "_col_subset = mcap_sdf.columns\n",
    "_col_subset.remove('mcap_date')\n",
    "_nan_counts_sdf = clsSCNR.count_column_nulls(\n",
    "    data=_unpivot_sdf,\n",
    "#     column_subset=_col_subset\n",
    ")\n",
    "\n",
    "# print(_nan_counts_sdf.count(),len(_nan_counts_sdf.columns))\n",
    "_nan_counts_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2173e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 18:27:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(_nan_counts_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a8af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

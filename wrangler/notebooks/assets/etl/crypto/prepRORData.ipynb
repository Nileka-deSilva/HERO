{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Derive Top N Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "dataPrep Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from utils.modules.etl.load import sparkDBwls as sdb\n",
    "from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from wrangler.modules.assets.etl import dataPrep as prep\n",
    "# from utils.modules.ml.timeseries import rollingstats as stats\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    sdb = importlib.reload(sdb)\n",
    "    scne = importlib.reload(scne)\n",
    "    prep = importlib.reload(prep)\n",
    "#     stats= importlib.reload(stats)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "# clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "clsSCNR=scne.Transformer(desc=__desc__)\n",
    "# clsStat=stats.RollingStats(desc=__desc__)\n",
    "clsPrep =prep.RateOfReturns(desc=__desc__)\n",
    "''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dd26e",
   "metadata": {},
   "source": [
    "## Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, retrieving data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1869:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74300 rows and 17 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_from_date = '2022-01-01'\n",
    "_to_date = '2022-01-31'\n",
    "# _query = \"select * from warehouse.mcap_past \"+\\\n",
    "#         f\"where mcap_date >= '{_from_date}' and \"+\\\n",
    "#         f\"mcap_date <= '{_to_date}'\"\n",
    "_query = \"select * from warehouse.mcap_past wmp \"+\\\n",
    "        f\"where wmp.mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "        f\"and wmp.mcap_value > 10000 \"\n",
    "_kwargs = {\n",
    "    \"TABLENAME\":'warehouse.mcap_past',\n",
    "    \"COLUMN\":'mcap_date',\n",
    "    \"FROMDATETIME\":_from_date,\n",
    "    \"TODATETIME\":_to_date,\n",
    "    \"PARTITIONS\":2,\n",
    "    \"AGGREGATE\":'avg',\n",
    "    \"LOGBASE\":'10',\n",
    "    \"PIVCOLUMNS\":['dxd','sofi','wsn','xmx','uqc','btr','unic','nex','noia',\n",
    "                  'hanu','aca','bbs','xvs','pnd','shake','stpl','dtx','tethys',\n",
    "                  'kyoko','boba','nlife','rare','eved','yfl','fkx','flixx',\n",
    "                  'drk','meto','glide','shr','tetu','mft','cmerge','shmn','tronpad']\n",
    "}\n",
    "\n",
    "# print(clsSpark.dbSchema)\n",
    "mcap_sdf = clsPrep.read_n_clean_mcap(query=_query,**_kwargs)\n",
    "# mcap_sdf = clsROR.read_n_clean_mcap(**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c031155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+--------------------+\n",
      "|mcap_past_pk|          mcap_date|asset_name|          mcap_value|\n",
      "+------------+-------------------+----------+--------------------+\n",
      "|       49194|2022-01-01 00:00:00|       sas|10621.17568446250...|\n",
      "|      169510|2022-01-01 00:00:00|      edao|10948.90171404940...|\n",
      "|       39374|2022-01-01 00:00:00|      scho|12262.57486049260...|\n",
      "|       17823|2022-01-01 00:00:00|     rigel|13036.58566444630...|\n",
      "|      156450|2022-01-01 00:00:00|      brtr|13346.88083840520...|\n",
      "+------------+-------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1877:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "mcap_sdf=mcap_sdf.sort(F.col('mcap_date'),F.col('mcap_value'))\n",
    "mcap_sdf=mcap_sdf.filter(F.col('mcap_past_pk').isNotNull())\n",
    "mcap_sdf.select(F.col('mcap_past_pk'),\n",
    "                F.col('mcap_date'),\n",
    "                F.col('asset_name'),\n",
    "                F.col('mcap_value'))\\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a90a",
   "metadata": {},
   "source": [
    "## Compute LogROR for all assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "358fa4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ror='NATLOG'\n",
    "_ror_col=None\n",
    "\n",
    "if _ror=='NATLOG':\n",
    "    _ror_col='log_ror'\n",
    "elif _ror=='SIMP':\n",
    "    _ror_col='simp_ror'\n",
    "else:\n",
    "    pass\n",
    "_kwargs={\n",
    "    \"PREVALCOLNAME\":'mcap_lag',\n",
    "    \"DIFFCOLNAME\":'mcap_diff',\n",
    "    \"RORCOLNAME\":_ror_col,\n",
    "}\n",
    "\n",
    "_mcap_log_ror, _ror_col = clsPrep.get_ror(\n",
    "    data=mcap_sdf,\n",
    "    ror_type=_ror,\n",
    "    num_col =\"mcap_value\",\n",
    "    part_col='asset_name',\n",
    "    date_col='mcap_date',\n",
    "    **_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44431e4f",
   "metadata": {},
   "source": [
    "## Write ROR data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0d4c1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------------+--------------------+--------------------+\n",
      "|mcap_past_pk|asset_name|          mcap_date|          mcap_value|             log_ror|\n",
      "+------------+----------+-------------------+--------------------+--------------------+\n",
      "|       91384|      btcs|2022-01-02 00:00:00|2785102.132352710...| 0.05284004754788873|\n",
      "|      202976|     sngls|2022-01-02 00:00:00|155996.7475973480...|0.059862920030564884|\n",
      "|      120739|      shdw|2022-01-02 00:00:00|364515.8216135790...| 0.23306536690933657|\n",
      "|       67523|      sybc|2022-01-02 00:00:00|79187.09948750640...| 0.38644877160434254|\n",
      "|      153610|     scriv|2022-01-02 00:00:00|122004.6214330040...|  0.3963025935129252|\n",
      "+------------+----------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validating upsert attributes and parameters ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql tip database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1996:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 71034 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_upsert_sdf=_mcap_log_ror.select( \n",
    "    F.col('mcap_past_pk'),\n",
    "    F.col('asset_name'),\n",
    "    F.col('mcap_date'),\n",
    "    F.col('mcap_value'),\n",
    "    F.col(_ror_col),\n",
    ")\\\n",
    "    .filter((F.col(_ror_col).isNotNull()))\n",
    "_upsert_sdf.show(n=5)\n",
    "\n",
    "_records=clsPrep.write_data_to_db(\n",
    "    data=_upsert_sdf,\n",
    "#     **kwargs,\n",
    ")\n",
    "print(\"Upserted %d records\" % _records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af39f3",
   "metadata": {},
   "source": [
    "# DEPRECATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f6f24",
   "metadata": {},
   "source": [
    "## Weighted Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a39609d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:49:03 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:49:08 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 115:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:49:13 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:51:38 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:51:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 133:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:51:48 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 137:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:51:50 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:53:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:53:28 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 157:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:53:32 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:53:34 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:55:48 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:55:52 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 181:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:55:57 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 193:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:57:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:57:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:58:05 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 203:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:58:08 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:58:08 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 213:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 09:58:11 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:00:47 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:00:51 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 228:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:00:55 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/01/28 10:00:57 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 246:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:02:05 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:02:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 252:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:02:17 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 256:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:02:19 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 270:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:03:16 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:03:21 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 276:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:03:26 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 280:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:03:27 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 294:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:04:23 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:04:28 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 300:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:04:34 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 304:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:04:36 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 318:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:06:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:06:31 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 328:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:06:40 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/01/28 10:06:43 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 342:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:08:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:09:04 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 348:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:09:09 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 352:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:09:12 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:09:13 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 362:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:09:16 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 371:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:10:55 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:10:59 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 377:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:11:04 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 381:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:11:06 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:11:07 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 391:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:11:09 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 400:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:12:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:12:30 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 406:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:12:34 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/01/28 10:12:36 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 415:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:12:37 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "23/01/28 10:12:38 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 429:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:13:49 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:13:53 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 435:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:13:57 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/01/28 10:14:00 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:14:00 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 449:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:14:03 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 458:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:22 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:26 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 464:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:31 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 468:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:33 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 473:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:34 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 478:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/28 10:15:36 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 484:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mcap_date': datetime.date(2022, 1, 6),\n",
       "  'asset_name': ['charg_coin', 'roge', 'neurotoken'],\n",
       "  'log_ror': [0.25040658289005097, 0.2300847277541936, 0.22315158781277547],\n",
       "  'weights': [0.5934458610550608, 0.11493828956433183, 0.2916158493806074],\n",
       "  'mcap_value': [Decimal('5220183.74772765300000000000'),\n",
       "   Decimal('2145261.20389485800000000000'),\n",
       "   Decimal('1332808.07022394360000000000')]},\n",
       " {'mcap_date': datetime.date(2022, 1, 7),\n",
       "  'asset_name': ['robotina', 'sentinel_group', 'gravity_finance'],\n",
       "  'log_ror': [0.9659091703444093, 0.415784053356342, 0.24974754544640954],\n",
       "  'weights': [0.6295957738354031, 0.05237806329355537, 0.3180261628710416],\n",
       "  'mcap_value': [Decimal('1018345.62651913000000000000'),\n",
       "   Decimal('5870017.17622237100000000000'),\n",
       "   Decimal('2031961.70641057660000000000')]},\n",
       " {'mcap_date': datetime.date(2022, 1, 8),\n",
       "  'asset_name': ['primecoin', 'neurochain', 'bitcoin_scrypt'],\n",
       "  'log_ror': [0.45615688364912843, 0.4011424725185939, 0.28212772816557075],\n",
       "  'weights': [0.543694041943098, 0.28722923986128157, 0.16907671819562034],\n",
       "  'mcap_value': [Decimal('1475785.78355717600000000000'),\n",
       "   Decimal('1414947.27726557060000000000'),\n",
       "   Decimal('1308454.50419590090000000000')]},\n",
       " {'mcap_date': datetime.date(2022, 1, 9),\n",
       "  'asset_name': ['leverj_gluon', 'onooks', 'falconx'],\n",
       "  'log_ror': [0.5012158266094365, 0.22671200830424368, 0.16952631379675884],\n",
       "  'weights': [0.5942044176776197, 0.19710672092724646, 0.2086888613951338],\n",
       "  'mcap_value': [Decimal('2260417.60434771400000000000'),\n",
       "   Decimal('6378494.16271787800000000000'),\n",
       "   Decimal('1081416.01318761400000000000')]},\n",
       " {'mcap_date': datetime.date(2022, 1, 10),\n",
       "  'asset_name': ['charg_coin', 'matryx', 'bitcoinpos'],\n",
       "  'log_ror': [0.6574680456259401, 0.2861116369010913, 0.20342319564693787],\n",
       "  'weights': [0.4453207926976063, 0.34821271988748376, 0.20646648741490986],\n",
       "  'mcap_value': [Decimal('1148729.34965566430000000000'),\n",
       "   Decimal('2246173.23459720500000000000'),\n",
       "   Decimal('14522857.41687161100000000000')]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cols={\n",
    "    \"NAMECOLUMN\":'asset_name',\n",
    "    \"DATECOLUMN\":'mcap_date',\n",
    "    \"RORCOLUMN\" :_log_col,\n",
    "    \"MCAPCOLUMN\":'mcap_value',\n",
    "    \"WEIGHTCOLUMN\":'weights',\n",
    "}\n",
    "_l_exp_wts,_cols_dict=clsMPT.get_weighted_mpt(\n",
    "    data=_mcap_log_ror,\n",
    "    cols_dict=_cols,\n",
    "#     date_col='mcap_date',\n",
    "#     val_col='log_ror',\n",
    "#     name_col='asset_name',\n",
    "    topN=3,\n",
    "    size=5,\n",
    "    **_kwargs,\n",
    ")\n",
    "# print(\"Dates: %s\" % str(_wr_dates))\n",
    "# print(\"Data : row=%d columns=%d\" % (_wr_data.count(),len(_wr_data.columns)))\n",
    "_l_exp_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97eaaeb",
   "metadata": {},
   "source": [
    "## Write MPT to MongoDB\n",
    "\n",
    "* Collection name = \"mpt.\"+date(YYYY-MM-DD)\n",
    "* document structure: \\_id, date, asset, mcap.value, mcap.weight, mcap.ror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccad5a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert 5 documents\n"
     ]
    }
   ],
   "source": [
    "_kwargs = {\n",
    "    \"DESTINDBNAME\":'tip-daily-mpt',\n",
    "    \"COLLPREFIX\" : 'mpt.for'\n",
    "}\n",
    "mpt_list_ = clsMPT.write_mpt_to_db(\n",
    "    mpt_data=_l_exp_wts,\n",
    "    cols_dict=_cols_dict,\n",
    "    **kwargs,\n",
    ")\n",
    "print(\"Upsert %d documents\" % len(mpt_list_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb99563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

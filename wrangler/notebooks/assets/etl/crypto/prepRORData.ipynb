{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Derive Top N Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "All functional SPARKNOSQLWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "dataPrep Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from utils.modules.etl.load import sparkDBwls as sdb\n",
    "from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from wrangler.modules.assets.etl import dataPrep as prep\n",
    "# from utils.modules.ml.timeseries import rollingstats as stats\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    sdb = importlib.reload(sdb)\n",
    "    scne = importlib.reload(scne)\n",
    "    prep = importlib.reload(prep)\n",
    "#     stats= importlib.reload(stats)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "# clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "clsSCNR=scne.Transformer(desc=__desc__)\n",
    "# clsStat=stats.RollingStats(desc=__desc__)\n",
    "clsPrep =prep.RateOfReturns(desc=__desc__)\n",
    "''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dd26e",
   "metadata": {},
   "source": [
    "## Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, retrieving data ...\n",
      "23/02/21 14:32:22 WARN Utils: Your hostname, FarmRaiderTester resolves to a loopback address: 127.0.1.1; using 192.168.124.15 instead (on interface enp2s0)\n",
      "23/02/21 14:32:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/02/21 14:32:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/21 14:32:24 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem Unable to get public no-arg constructor\n",
      "23/02/21 14:32:24 WARN FileSystem: java.lang.NoClassDefFoundError: com/google/api/client/auth/oauth2/Credential\n",
      "23/02/21 14:32:24 WARN FileSystem: java.lang.ClassNotFoundException: com.google.api.client.auth.oauth2.Credential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 14:32:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/21 14:32:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74300 rows and 17 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_from_date = '2022-01-01'\n",
    "_to_date = '2022-01-31'\n",
    "# _query = \"select * from warehouse.mcap_past \"+\\\n",
    "#         f\"where mcap_date >= '{_from_date}' and \"+\\\n",
    "#         f\"mcap_date <= '{_to_date}'\"\n",
    "_query = \"select * from warehouse.mcap_past wmp \"+\\\n",
    "        f\"where wmp.mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "        f\"and wmp.mcap_value > 10000 \"\n",
    "_kwargs = {\n",
    "    \"TABLENAME\":'warehouse.mcap_past',\n",
    "    \"COLUMN\":'mcap_date',\n",
    "    \"FROMDATETIME\":_from_date,\n",
    "    \"TODATETIME\":_to_date,\n",
    "    \"PARTITIONS\":2,\n",
    "    \"AGGREGATE\":'avg',\n",
    "    \"LOGBASE\":'10',\n",
    "    \"PIVCOLUMNS\":['dxd','sofi','wsn','xmx','uqc','btr','unic','nex','noia',\n",
    "                  'hanu','aca','bbs','xvs','pnd','shake','stpl','dtx','tethys',\n",
    "                  'kyoko','boba','nlife','rare','eved','yfl','fkx','flixx',\n",
    "                  'drk','meto','glide','shr','tetu','mft','cmerge','shmn','tronpad']\n",
    "}\n",
    "\n",
    "# print(clsSpark.dbSchema)\n",
    "mcap_sdf = clsPrep.read_n_clean_mcap(query=_query,**_kwargs)\n",
    "# mcap_sdf = clsROR.read_n_clean_mcap(**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c031155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "|mcap_past_pk|          mcap_date|asset_name|          mcap_value|           log_ror|\n",
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "|       49195|2022-01-02 00:00:00|       sas|10065.83106084450...|0.6666558637706426|\n",
      "|      169511|2022-01-02 00:00:00|      edao|11296.84312717820...|0.7089117639589353|\n",
      "|       39375|2022-01-02 00:00:00|      scho|12291.45593138100...|0.6943239878505439|\n",
      "|       17824|2022-01-02 00:00:00|     rigel|12864.00343692410...|0.6865061778350110|\n",
      "|      165997|2022-01-02 00:00:00|     distx|14332.52767831190...|0.6844287855620943|\n",
      "+------------+-------------------+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "mcap_sdf=mcap_sdf.sort(F.col('mcap_date'),F.col('mcap_value'))\n",
    "mcap_sdf=mcap_sdf.filter(F.col('mcap_past_pk').isNotNull())\n",
    "mcap_sdf.select(F.col('mcap_past_pk'),F.col('mcap_date'),F.col('asset_name'),\n",
    "                F.col('mcap_value'),F.col('log_ror'))\\\n",
    "    .filter(F.col('log_ror').isNotNull())\\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a90a",
   "metadata": {},
   "source": [
    "## Compute LogROR for all assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358fa4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 79:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_ror='SIMP'\n",
    "_ror_col=None\n",
    "\n",
    "if _ror=='NATLOG':\n",
    "    _ror_col='log_ror'\n",
    "elif _ror=='SIMP':\n",
    "    _ror_col='simp_ror'\n",
    "else:\n",
    "    pass\n",
    "_kwargs={\n",
    "    \"PREVALCOLNAME\":'mcap_lag',\n",
    "    \"DIFFCOLNAME\":'mcap_diff',\n",
    "    \"RORCOLNAME\":_ror_col,\n",
    "}\n",
    "\n",
    "_mcap_log_ror, _ror_col = clsPrep.get_ror(\n",
    "    data=mcap_sdf,\n",
    "    ror_type=_ror,\n",
    "    num_col =\"mcap_value\",\n",
    "    part_col='asset_name',\n",
    "    date_col='mcap_date',\n",
    "    **_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44431e4f",
   "metadata": {},
   "source": [
    "## Write ROR data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d4c1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------------+--------------------+----------+\n",
      "|mcap_past_pk|asset_name|          mcap_date|          mcap_value|  simp_ror|\n",
      "+------------+----------+-------------------+--------------------+----------+\n",
      "|       91384|      btcs|2022-01-02 00:00:00|2785102.132352710...|-17.429416|\n",
      "|      202976|     sngls|2022-01-02 00:00:00|155996.7475973480...|-15.209829|\n",
      "|      120739|      shdw|2022-01-02 00:00:00|364515.8216135790...| -2.810053|\n",
      "|       67523|      sybc|2022-01-02 00:00:00|79187.09948750640...| -1.119790|\n",
      "|      153610|     scriv|2022-01-02 00:00:00|122004.6214330040...| -1.056263|\n",
      "+------------+----------+-------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Validating upsert attributes and parameters ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql tip database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 71034 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_upsert_sdf=_mcap_log_ror.select( \n",
    "    F.col('mcap_past_pk'),\n",
    "    F.col('asset_name'),\n",
    "    F.col('mcap_date'),\n",
    "    F.col('mcap_value'),\n",
    "    F.col(_ror_col),\n",
    ")\\\n",
    "    .filter((F.col(_ror_col).isNotNull()))\n",
    "_upsert_sdf.show(n=5)\n",
    "\n",
    "_records=clsPrep.write_data_to_db(\n",
    "    data=_upsert_sdf,\n",
    "#     **kwargs,\n",
    ")\n",
    "print(\"Upserted %d records\" % _records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb99563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
